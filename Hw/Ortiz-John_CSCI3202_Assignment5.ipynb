{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"John Ortiz\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372b2e64a091bc5c1e72fcc5f697fe12",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202, Spring 2021:  Assignment 5  \n",
    "\n",
    "Shortcuts:  [top](#top) -- [1](#p1) | [1a](#p1a) | [1b](#p1b) | [1c](#p1c) | [1d](#p1d) | [1e](#p1e) | [1f](#p1f) | [1g](#p1g) -- [2](#p2) | [2a](#p2a) | [2b](#p2b) | [2c](#p2c) | [2d](#p2d) | [2e](#p2e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c31566edfa1fdd0af96343452430d5f3",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment Overview\n",
    "\n",
    "This assignment is an exercise in implementing and analyzing Markov Decision Processes (MDPs). Problem 1 asks you to code a solution to a specific scenario, while Problem 2 is a conceptual question which asks you to describe an MDP problem of your own design.\n",
    "\n",
    "Here's a summary of the tasks required and the associated points:\n",
    "\n",
    "| Problem #  | Tasks                                                  | Points  |\n",
    "|:---        |:---                                                    |:---:    |\n",
    "| 1a         | Code: Complete implementation of `MDP` class           | 10      |\n",
    "| 1b         | Code: Implement `value_iteration` and `find_policy`    | 5       |\n",
    "| 1c         | Code and create: Generate and illustrate optimal path  | 5       |\n",
    "| 1d         | Written: analyze policy                                | 5       |\n",
    "| 1e         | Code: adjust non-terminal rewards                      | 5       |\n",
    "| 1f         | Code and write: adjust terminal rewards                | 5       |\n",
    "| 1g         | Written: analyze transition model                      | 5       |\n",
    "| 2a         | Written: define problem                                | 4       |\n",
    "| 2b         | Written: define states                                 | 4       |\n",
    "| 2c         | Written: define reward                                 | 4       |\n",
    "| 2d         | Written: define actions and transition                 | 4       |\n",
    "| 2e         | Written: define optimal policy                         | 4       |\n",
    "| Total      |                                                        | 60      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# add any imports you may need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c608e4fde84b1104aa33382b2642da7",
     "grade": false,
     "grade_id": "1-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 1: Navigating an awkward situation with grace and poise\n",
    "\n",
    "<img src='https://www.explainxkcd.com/wiki/images/5/5f/interaction.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Suppose you are at a social event where you would like to avoid any interaction with a large number of the other attendees. It's not that you don't like them, it's just that you don't like *talking to* them. A few of your good friends are also in attendance, but they are tucked away in a corner. The rectangular room in which the event is being held spans gridcells at $x=1,2,\\ldots, 6$ and $y=1,2,\\ldots, 5$. At the eastern edge ($x=6$) of this first floor room, there is a balcony, with a 6-foot drop. If the event becomes unbearably awkward, you can jump off the balcony and run away. Of course, this might hurt a little bit, so we should incorporate this into our reward structure.\n",
    "\n",
    "The terminal states and rewards associated with them are given in the diagram below. The states are represented as $(x,y)$ tuples. The available actions in non-terminal states include moving exactly 1 unit North (+y), South (-y), East (+x) or West (-x), although you should not include walking into walls, because that would be embarrassing in front of all these other people. Represent actions as one of 'N', 'S', 'E', or 'W'. For now, assume all non-terminal states have a default reward of -0.01, and use a discount factor of 0.99.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Use the following transition model for this decision process, if you are trying to move from state $s$ to state $s'$:\n",
    "* you successfully move from $s$ to $s'$ with probability 0.6\n",
    "* the remaining 0.4 probability is spread equally likely across state $s$ **and** all adjacent (N/S/E/W) states except for $s'$. Note that this does not necessarily mean that all adjacent states have 0.1, because some states do not have 4 adjacent states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "823e20cadbdae74fd72ea6227b26eb1c",
     "grade": false,
     "grade_id": "1a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1a'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1a) - 10 points\n",
    "\n",
    "Complete the `MDP` class below. The docstring comments provide some desired specifications. You may add additional methods or attributes, if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e28782999a0a5c35e0461daeb267be5c",
     "grade": false,
     "grade_id": "mdp-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, nrow, ncol, terminal_states, default_reward, df):\n",
    "        '''Create/store the following attributes:\n",
    "        self.states -- a list of all the states as (x,y) tuples\n",
    "        self.terminal_states -- a dictionary with terminal state keys, and rewards as values\n",
    "        self.default_reward -- the reward for being in any non-terminal state\n",
    "        self.df -- discount factor\n",
    "        ... and anything else you decide will be useful!\n",
    "        '''\n",
    "        \n",
    "        self.states=  []\n",
    "        for x in range(6):\n",
    "            for y in range(5):\n",
    "                state = (x+1,y+1)\n",
    "                self.states.append(state)\n",
    "        self.terminal_states =terminal_states #we could need to change to be for loop\n",
    "        self.default_reward = default_reward\n",
    "        self.df = df\n",
    "        \n",
    "        #possibleAction\n",
    "        self.possibleActions = ['N','S','E','W',None]\n",
    "        #raise NotImplementedError()\n",
    "    #helper for actions\n",
    "    def isInBounds(self,newX,newY):\n",
    "        #return true if newX and newY are in boundaries\n",
    "        isIn = True\n",
    "        if((newX<1) or (newX>6)):\n",
    "            isIn = False\n",
    "        if((newY<1) or (newY>5)):\n",
    "            isIn = False\n",
    "        return isIn\n",
    "    def actions(self, state):\n",
    "        '''Return a list of available actions from the given state.\n",
    "        Possible actions are 'N','S','E','W'\n",
    "        [None] are the actions available from a terminal state.\n",
    "        '''\n",
    "        #first check if it is terminal state\n",
    "        if(state in self.terminal_states):\n",
    "            return [None]\n",
    "        #possibleActions = ['N','S','E','W']\n",
    "        newStates=[(state)]*len(self.possibleActions)\n",
    "        finalActions=[]\n",
    "        #for each possible action check if (newx,newy) outbound if is do add to actionlist:(x,y) -> (x,y+1)\n",
    "        for i in range(len(newStates)):\n",
    "            if(i==0):\n",
    "                #north -> (x,y+1)\n",
    "                newState= list(newStates[i])\n",
    "                newState[1] = newState[1]+1\n",
    "                if(self.isInBounds(newState[0],newState[1])):\n",
    "                    finalActions.append(self.possibleActions[i])\n",
    "            elif(i==1):\n",
    "                #south -> (x,y-1)\n",
    "                newState= list(newStates[i])\n",
    "                newState[1] = newState[1]-1\n",
    "                if(self.isInBounds(newState[0],newState[1])):\n",
    "                    finalActions.append(self.possibleActions[i])\n",
    "            elif(i==2):\n",
    "                #east -> (x+1,y)\n",
    "                newState= list(newStates[i])\n",
    "                newState[0] = newState[0]+1\n",
    "                if(self.isInBounds(newState[0],newState[1])):\n",
    "                    finalActions.append(self.possibleActions[i])\n",
    "            elif(i==3):\n",
    "                #west -> (x-1,y)\n",
    "                newState= list(newStates[i])\n",
    "                newState[0] = newState[0]-1\n",
    "                if(self.isInBounds(newState[0],newState[1])):\n",
    "                    finalActions.append(self.possibleActions[i])\n",
    "            #elif(i==4):\n",
    "            #    finalActions.append(None)\n",
    "        return finalActions\n",
    "    def reward(self, state):\n",
    "        '''Return the reward for being in the given state'''\n",
    "        \n",
    "        #check if given state is terminal state\n",
    "        if(state in self.terminal_states):\n",
    "            #terminal reward\n",
    "            reward = self.terminal_states.get(state)\n",
    "        else:\n",
    "            #default reward\n",
    "            reward = self.default_reward\n",
    "        #raise NotImplementedError()\n",
    "        return reward\n",
    "    def result(self, state, action):\n",
    "        '''Return the resulting state (as a tuple) from doing the given\n",
    "        action in the given state, without uncertainty. Uncertainty\n",
    "        is incorporated into the transition method.\n",
    "        state -- a tuple representing the current state\n",
    "        action -- one of N, S, E or W, as a string\n",
    "        '''\n",
    "               \n",
    "        \n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "        actionIndex = self.possibleActions.index(action)\n",
    "        \n",
    "        if(actionIndex==0):\n",
    "            #north -> (x,y+1)\n",
    "            newState= list(state)\n",
    "            newState[1] = newState[1]+1\n",
    "            if(self.isInBounds(newState[0],newState[1])):\n",
    "                return (newState[0],newState[1])\n",
    "        elif(actionIndex==1):\n",
    "            #south -> (x,y-1)\n",
    "            newState= list(state)\n",
    "            newState[1] = newState[1]-1\n",
    "            if(self.isInBounds(newState[0],newState[1])):\n",
    "                return (newState[0],newState[1])\n",
    "        elif(actionIndex==2):\n",
    "            #east -> (x+1,y)\n",
    "            newState= list(state)\n",
    "            newState[0] = newState[0]+1\n",
    "            if(self.isInBounds(newState[0],newState[1])):\n",
    "                return (newState[0],newState[1])\n",
    "        elif(actionIndex==3):\n",
    "            #west -> (x-1,y)\n",
    "            newState= list(state)\n",
    "            newState[0] = newState[0]-1\n",
    "            if(self.isInBounds(newState[0],newState[1])):\n",
    "                return (newState[0],newState[1])\n",
    "        \n",
    "    \n",
    "                \n",
    "    def transition(self, state, action):\n",
    "        '''Return a list of (probability, next_state) associated\n",
    "        with the possibilities of taking the given action from the given state.\n",
    "        '''\n",
    "        #you successfully move from  𝑠  to  𝑠′  with probability 0.6 the remaining 0.4 probability is \n",
    "        #spread equally likely across state  𝑠  and all adjacent (N/S/E/W) states except for  𝑠′ . \n",
    "        #Note that this does not necessarily mean that all adjacent states have 0.1, \n",
    "        #because some states do not have 4 adjacent states.\n",
    "        if action is None:\n",
    "            # This happens for a terminal state\n",
    "            return [(0, state)]\n",
    "        else:\n",
    "            # Not a terminal state\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            #grab available actions\n",
    "            avActions = self.actions(state)\n",
    "            avActions.append(None)\n",
    "            indexOfAction = avActions.index(action)\n",
    "            p = [1]*len(avActions)\n",
    "            p[indexOfAction]= .6\n",
    "            evenProb = .4/(len(avActions)-1)\n",
    "            \n",
    "            #sanity check\n",
    "            #print(avActions)\n",
    "            #print(p)\n",
    "            #print(evenProb)\n",
    "            \n",
    "            #update rest of probabilities\n",
    "            for i in range(len(avActions)):\n",
    "                if(i!=indexOfAction):\n",
    "                    p[i]= evenProb\n",
    "            #print(p)\n",
    "            \n",
    "            transitionList =[]\n",
    "            #create new next state for each action in avActions\n",
    "            for i in range(len(avActions)):\n",
    "                if(avActions[i]!= None):\n",
    "                    newState = self.result(state,avActions[i])\n",
    "                    transitionList.append((p[i],newState))\n",
    "                else:\n",
    "                    transitionList.append((p[i],state))\n",
    "            \n",
    "            #sPrime =np.random.choice(avActions,p=p)\n",
    "            #avActions.append()\n",
    "            #raise NotImplementedError()\n",
    "            \n",
    "            #print(transitionList)\n",
    "            return transitionList\n",
    "    def expected_utility(self, next_states, cur_util):\n",
    "        '''Return the expected utility given generated list of possible \n",
    "        next states and the current utility, which is a dictionary of the form {state : utility}\n",
    "        '''\n",
    "        \n",
    "        sumUtil=0\n",
    "        #print(\"Next States:\"+str(next_states))\n",
    "        for possibleState in next_states:\n",
    "            #print(\"possibleState: \"+str(possibleState))\n",
    "            prob = possibleState[0]\n",
    "            util_of_state =cur_util.get(possibleState[1])\n",
    "            #print(\"multiplying: \"+str(prob)+\", \"+str(util_of_state) )\n",
    "            sumUtil += (prob*util_of_state)\n",
    "        return sumUtil\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75dc0b42cb21d9584de2037444b26d51",
     "grade": false,
     "grade_id": "1a-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1a) tests\n",
    "\n",
    "Note that these are non-exhaustive, because there is some flexibility in how the `transition` method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e36e4589b1131c2c413d7a804bf7f4c",
     "grade": true,
     "grade_id": "1a-test-simple",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "nrow = 3\n",
    "ncol = 3\n",
    "default_reward = -0.2\n",
    "discount = 0.5\n",
    "terminal = {(1,3):-1, (1,2):2}\n",
    "mdp_simple = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "actions1 = set(mdp_simple.actions((2,2)))\n",
    "assert (actions1 == {'N','S','E','W'}), \"Expected set of actions is {'N','S','E','W'}, your code returned: %s\" % actions1\n",
    "\n",
    "actions2 = set(mdp_simple.actions((1,1)))\n",
    "assert (actions2 == {'N','E'}), \"Expected set of actions is {'N','E'}, your code returned: %s\" % actions2\n",
    "\n",
    "actions3 = set(mdp_simple.actions((1,2)))\n",
    "assert (actions3 == {None}), \"Expected set of actions is {None}, your code returned: %s\" % actions3\n",
    "\n",
    "reward1 = mdp_simple.reward((1,2))\n",
    "assert (reward1 == 2), \"Expected reward is 2, your code returned: %f\" % reward1\n",
    "\n",
    "reward2 = mdp_simple.reward((2,2))\n",
    "assert (reward2 == -0.2), \"Expected reward is -0.2, your code returned: %f\" % reward2\n",
    "\n",
    "result1 = mdp_simple.result((1,1), 'N')\n",
    "assert (result1 == (1,2)), \"Expected result is (1,2), your code returned: %s\" % (result1,)\n",
    "\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6b6124959228ec56ece931028694a1",
     "grade": true,
     "grade_id": "1a-test-problem",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "# set values from problem statement\n",
    "nrow = 5\n",
    "ncol = 6\n",
    "default_reward = -0.01\n",
    "discount = 0.99\n",
    "terminal = {(1,3):-1, (1,4):2, (1,5):2, (2,1):-1, (3,1):-1, (3,4):-1, (3,5):1,\n",
    "            (4,3):-1, (4,4):-1, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "mdp_p1 = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "# Find the expected utility of walking N from (1,1):\n",
    "util_old = {s : s[0]+s[1] for s in mdp_p1.states}\n",
    "\n",
    "next_states = mdp_p1.transition((1,1), 'N')\n",
    "assert (len(next_states) == 3), \"Expected 3 possible next states, your code returned: %d\" % len(next_states)\n",
    "\n",
    "exp_util = mdp_p1.expected_utility(next_states, util_old)\n",
    "assert (exp_util == 2.8), \"Expected utility should be 2.8, your code returned %f\" % exp_util\n",
    "\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1171844cdb0baf1260559a6f58a18477",
     "grade": false,
     "grade_id": "1b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1b'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1b) - 5 points\n",
    "\n",
    "Implement value iteration to calculate the utilities for each state.  Also implement a function that takes as arguments an `MDP` object and a dictionary of state-utility pairs (key-value) and returns a dictionary for the optimal policy.  The optimal policy dictionary should have state tuples as keys and the optimal move (None, N, S, E or W) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c8c83a0cd970ca44ba24d8f8715781",
     "grade": false,
     "grade_id": "value-iter-find-policy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, tol=1e-3):\n",
    "    # TODO: \n",
    "    # 1. initialize utility for all states to 0\n",
    "    # 2. for each state on the board\n",
    "    #    2.1. calculate expected utility for each possible next state\n",
    "    #    2.2. find best utility out of possible expected utilities\n",
    "    #    2.3. define new utility of the state\n",
    "    # 3. Repeat 2 until problem is converged\n",
    "    utility = {s : 0 for s in mdp.states}\n",
    "    tolerence_Met=10000\n",
    "\n",
    "    while(tolerence_Met > tol):\n",
    "        snapUtility = utility.copy()\n",
    "        for i in range(len(mdp.states)):\n",
    "            state=mdp.states[i]\n",
    "            possibleActions = mdp.actions(state)\n",
    "            #print(state)\n",
    "            #print(\"---------------\")\n",
    "            possibleUtilities=[] #store all possible expectUtilities from a given action\n",
    "            \n",
    "            for action in possibleActions:\n",
    "                if(action != None):\n",
    "                    nextStates= mdp.transition(state,action)\n",
    "\n",
    "                    expectUtility = mdp.expected_utility(nextStates, snapUtility)\n",
    "                    #print(action,nextStates,expectUtility)\n",
    "                    possibleUtilities.append(expectUtility)\n",
    "                else:\n",
    "                    possibleUtilities.append(0)\n",
    "            maxExpectUtil = max(possibleUtilities)\n",
    "            \n",
    "            newUtil =(maxExpectUtil*mdp.df + mdp.reward(state))\n",
    "            #print(\"newUtil: \"+str(newUtil))\n",
    "            \n",
    "            u1 ={state : newUtil}\n",
    "            \n",
    "            snapUtility.update(u1)\n",
    "        #update utility with new version of utility\n",
    "        newUtilList = list(snapUtility.values())\n",
    "        oldUtilList = list(utility.values())\n",
    "        diffList = [abs(newUtilList[x]-oldUtilList[x]) for x in range(len(newUtilList))] #gets differences\n",
    "        tolerence_Met = max(diffList) #gets biggest diff\n",
    "        utility = snapUtility\n",
    "        \n",
    "    return utility   \n",
    "    \n",
    "\n",
    "def find_policy(mdp, utility):\n",
    "    '''Return a dictionary containing the best policy for each state s,\n",
    "    of the form {s : s_policy}\n",
    "    '''\n",
    "    bestPolicies = {s : None for s in mdp.states}\n",
    "    for i in range(len(mdp.states)):\n",
    "            state=mdp.states[i]\n",
    "            if(state not in mdp.terminal_states):\n",
    "                possibleActions = mdp.actions(state)\n",
    "                possibleUtilities=[]\n",
    "                actionList=[]\n",
    "                #print(state)\n",
    "                #print(\"---------------\")\n",
    "                for action in possibleActions:\n",
    "                    nextStates= mdp.transition(state,action)\n",
    "                    expectUtility = mdp.expected_utility(nextStates, utility)\n",
    "                    #print(action,nextStates,expectUtility)\n",
    "                    possibleUtilities.append(expectUtility)\n",
    "                    actionList.append(action)\n",
    "                maxExpectUtil = max(possibleUtilities)\n",
    "                indexMaxUtil = possibleUtilities.index(maxExpectUtil)\n",
    "                #print(\"maxValue:\"+str(maxExpectUtil)+str(actionList[indexMaxUtil]))\n",
    "                \n",
    "                u1 ={state : actionList[indexMaxUtil]}\n",
    "            \n",
    "                bestPolicies.update(u1)\n",
    "            \n",
    "    return bestPolicies\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f14e15e8c0ad815923f021a8797e81e",
     "grade": false,
     "grade_id": "1b-tests-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1b) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54d20c4e3b584c7e7863d35660db1f6c",
     "grade": true,
     "grade_id": "1b-tests-asserts",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test cases: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "utility = value_iteration(mdp_p1, tol=1e-6)\n",
    "policy = find_policy(mdp_p1, utility)\n",
    "\n",
    "util1 = utility[(1,5)]\n",
    "assert (util1 == 2), \"Expected utility of 2, your code returned %f\" % util1\n",
    "\n",
    "util2 = utility[(6,1)]\n",
    "assert (util2 == -5), \"Expected utility of -5, your code returned %f\" % util2\n",
    "\n",
    "util3 = round(utility[(2,5)],2)\n",
    "assert (util3 == 1.74), \"Expected utility of 1.74, your code returned %f\" % util3\n",
    "\n",
    "util4 = round(utility[(5,3)],2)\n",
    "assert (util4 == -1.39), \"Expected utility of -1.39, your code returned %f\" % util4\n",
    "\n",
    "policy1 = policy[(2,4)]\n",
    "assert (policy1 == 'W'), \"Expected policy is 'W', your code returned %s\" % policy1\n",
    "\n",
    "policy2 = policy[(1,1)]\n",
    "assert (policy2 == 'N'), \"Expected policy is 'N', your code returned %s\" % policy2\n",
    "\n",
    "print(\"Passed test cases: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1b6b83bb256c438e69fc7bb280e5a80",
     "grade": false,
     "grade_id": "1c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1c'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1c) - 5 points\n",
    "\n",
    "If we enter the room at $s_0$, what is the optimal path for us to follow? Complete the following function to generate the sequence of states along the path. If we start in state $s_0$, then your output should be in the form $[s_0, s_1, s_2, ... , s_{term}]$ where $s_{term}$ is a terminal state. Set your tolerance for value iteration to be $10^{-6}$\n",
    "\n",
    "Additionally, create a graphic to illustrate this policy pathway, either by generating a plot in Python or by uploading a hand-drawn image and including a link to it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "740f51d6586bfb8d597b3afe8e46b108",
     "grade": false,
     "grade_id": "1c-answer-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_optimal_path(mdp, state):\n",
    "    '''Generate list of states visited along the optimal path given an MDP\n",
    "    instance and the starting state\n",
    "    '''\n",
    "    utility = value_iteration(mdp, tol=1e-6)\n",
    "    policy = find_policy(mdp, utility)\n",
    "    optimalPath =[]\n",
    "    currentState = state\n",
    "    while (currentState not in mdp.terminal_states):\n",
    "        bestPolicy = policy[currentState]\n",
    "        optimalPath.append(currentState)\n",
    "        currentState =mdp.result(currentState, bestPolicy)\n",
    "    optimalPath.append(currentState)#add terminal state\n",
    "    return optimalPath\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = find_optimal_path(mdp_p1, (5,3))\n",
    "path1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "085e7a8e272b36a76e8ce82e1537152e",
     "grade": false,
     "grade_id": "1c-answer-graphic",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Put a link to your graphic below for the path from (5,3). You can find an [example image here](http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp_path.png) with the optimal path starting from (5,1). **Please include a link rather than attaching a file**. This can be a link to your file in Google Drive, with the permissions set to public. The graders will not ask for permissions! The syntax for including a link in markdown is `[link text](url.com/to/your/image)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bd08159d2a99ee23492b15403f5e025",
     "grade": true,
     "grade_id": "1c-graphic",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "[Link to Optimal Path From (5,3)](https://drive.google.com/file/d/1dcGCc7FghJAyEWyM4Sn7BMH_9RRI1LPU/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f5c9f5250a8cb8db949d1166219cf06",
     "grade": true,
     "grade_id": "1c-tests-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed tests: 3 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "path1 = find_optimal_path(mdp_p1, (5,3))\n",
    "assert (path1 == [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path1\n",
    "\n",
    "path2 = find_optimal_path(mdp_p1, (5,1))\n",
    "assert (path2 == [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path2\n",
    "\n",
    "path3 = find_optimal_path(mdp_p1, (1,1))\n",
    "assert (path3 == [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path3\n",
    "\n",
    "print(\"Passed tests: 3 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b468abfa8603618ab21ea57d3a26d564",
     "grade": false,
     "grade_id": "1d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1d'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1d) - 5 points\n",
    "\n",
    "From (3,2) the optimal move is to walk West. If we are trying to go talk to our friends in the Northwest corner, why would we rather do this than walk North first, then West?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75467f69d87838440cb5b2a3dd342a34",
     "grade": true,
     "grade_id": "1d-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In context, of the MDP, it is because there is a negative reward if we were to take action North. In the context of the problem it would be because there is a person we do not know in the North direction and if we end up going there than we would be forced to have an anxiety filled conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "745fc8b0c5373d58b894072f14d2bcf8",
     "grade": false,
     "grade_id": "1e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1e'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1e) - 5 points\n",
    "\n",
    "How painfully awkward do you need to set the default reward for non-terminal states before the optimal move at (5,1) becomes jumping off the balcony immediately and running away? Implement the following function which returns the reward where the policy for (5,1) is to jump off the balcony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83a84df7c9004254b67cada546239f68",
     "grade": false,
     "grade_id": "1e-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_non_terminal_reward():\n",
    "    nrow = 5\n",
    "    ncol = 6\n",
    "    default_reward = -0.01\n",
    "    discount = 0.99\n",
    "    terminal = {(1,3):-1, (1,4):2, (1,5):2, (2,1):-1, (3,1):-1, (3,4):-1, (3,5):1,\n",
    "            (4,3):-1, (4,4):-1, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "    \n",
    "    for reward in np.arange(-0.01, -3, -0.01):\n",
    "        mdp = MDP(nrow, ncol, terminal, reward, discount)\n",
    "        path1 = find_optimal_path(mdp, (5,1))\n",
    "        #print(reward,path1)\n",
    "        if(path1 == [(5, 1), (6, 1)]):\n",
    "            #print(reward)\n",
    "            return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4346aaa5a01e06b338b3482b225aaa30",
     "grade": true,
     "grade_id": "1e-tests-assert",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "reward1 = find_non_terminal_reward()\n",
    "assert (reward1 == -2.09), \"The expected reward is -2.09, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Test passed: 5 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e194e2f330afbd4fe1437648f6f86723",
     "grade": false,
     "grade_id": "1f-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1f'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1f) - 5 points\n",
    "\n",
    "In **1e** we assumed a certain level of loss (negative reward) just for being present.  But a more realistic approach might be to instead change the reward structure for the terminal states. Consider the terminal states with -1 reward in the default model. Let $R^*$ denote the reward associated with these states. How low does $R^*$ need to be in order for us to immediately jump off the balcony and run away? Use the default non-terminal state reward of -0.01. Implement the following function to return the value of $R^*$ which leads to a policy of jumping off the balcony at (5,1). Write a few sentences interpreting your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03cd66008c7b709512677ede48059600",
     "grade": false,
     "grade_id": "1f-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_terminal_reward():\n",
    "    nrow = 5\n",
    "    ncol = 6\n",
    "    default_reward = -0.01\n",
    "    discount = 0.99\n",
    "    \n",
    "    for rstar in np.arange(-6, -12, -0.01):\n",
    "        # TODO:\n",
    "        # 1. set the reward of the terminal nodes \n",
    "        \n",
    "        # 2. define MDP with appropriate parameters\n",
    "        # 3. find policy for state (5,1)\n",
    "        # 4. return R* if the policy for (5,1) is to jump off the balcony\n",
    "        terminal = {(1,3):rstar, (1,4):2, (1,5):2, (2,1):rstar, (3,1):rstar, (3,4):rstar, (3,5):1,\n",
    "            (4,3):rstar, (4,4):rstar, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "        mdp = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "        path1 = find_optimal_path(mdp, (5,1))\n",
    "        #print(reward,path1)\n",
    "        if(path1 == [(5, 1), (6, 1)]):\n",
    "            #print(reward)\n",
    "            return rstar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.39"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward1 = round(find_terminal_reward(),2)\n",
    "reward1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7008fb02d7df7ba16c7882585e9b890d",
     "grade": false,
     "grade_id": "1f-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1f) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5230fc87068ca9643b6d7333441b715",
     "grade": true,
     "grade_id": "1f-test-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test: 3 points\n"
     ]
    }
   ],
   "source": [
    "reward1 = round(find_terminal_reward(),2)\n",
    "assert (reward1 == -11.39), \"Expected reward is -11.39, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Passed test: 3 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b7fd68f561b33ad94ed8a286a7eb3d6",
     "grade": false,
     "grade_id": "1f-reflection-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a few sentences with your interpretation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12e68d9c45421d6fd097011fd0b9f8bd",
     "grade": true,
     "grade_id": "1f-answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The reward of -11.39 is a pretty negative reward compare to a -5 of jumping off the balcony. In the context of the problem one must have a crippling anxiety to have such a negative reward for talking to strangers. It would make more sense for such a negative on like a particular person you specifically do not want to see like an ex. In terms of the MDP optimal policy it changed policies when it was not worth the risk of taking the optimal path to a positive reward state as going off direction has a .4 probability: which is why it is more dangerous to run into one of these anxiety promoting partygoers than just take the balcony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d2e45eceb59f1d36e0c528cd7faf22b",
     "grade": false,
     "grade_id": "1g-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1g'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1g) - 5 points\n",
    "\n",
    "Given the problem context, write a few sentences about why this is or is not an appropriate transition model. Include an interpretation of the terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a884715306f6da7b8d2b7fd566f7732e",
     "grade": true,
     "grade_id": "1g-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I think the transition model is appropiate as there is alot of moving parts at a party. Setting out to move across a room during a party, I believe that the .4 dedicated to not making it to the intended next state is justified. At a party, you really can not control alot. There is a chance you could run in people you know or even if you do not know them they could know you. If one is of legal age, inebriation could also acount for a part of why we did not move to the next state. I think the rewards for the terminal states describing running into some one you do not want to talk to have a good set reward of -1. It would not be the end of the world to have a two minute slightly okward conversation. Alse the positive terminal states make sense for being positive, but I do not understand why one of my friends is only +1. Do I like them less? The terminal states for jumping of the balcony, I believe have to small of a negative reward. Personally, I think jumping of the balcony should have at least a negative 15 reward.As I think jumping off the balcony might have people questioning one's sanity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5038f4053461860ec0728cc4626f059b",
     "grade": false,
     "grade_id": "2-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "<a id='p2'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 2: Define your very own MDP\n",
    "\n",
    "For this problem, you do not need to write any code, but rather communicate your ideas clearly using complete sentences and descriptions of the concepts the questions ask about. You can, of course, include some pseudocode if it helps, but that is not strictly necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12e0af4a88a181762bd42ae609e93cf",
     "grade": false,
     "grade_id": "2a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2a'></a>\n",
    "\n",
    "## (2a) - 4 points\n",
    "\n",
    "Describe something you think would be interesting to model using a Markov decision process.  Be **creative** - do not use any examples from your homework, class, or the textbook, and if you are working with other students, please **come up with your own example**. There are so, SO many possible answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d13a848d94d3a2552c856f613b1fd2d",
     "grade": true,
     "grade_id": "2a-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Imagine, that you heard their would be a great snowfall this Saturday. So you set off to make the biggest snowball. On that Saturday you grab your outdoor heater to keep your self warm, however it is teathered by an extension cord for power. So you decide it would be best to just make trips back to moderate your temperature. Lets use a Markov decision process to model it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33940776445caeaffd19100073cef2d0",
     "grade": false,
     "grade_id": "2b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2b'></a>\n",
    "\n",
    "## (2b) - 4 points\n",
    "\n",
    "What are the states associated with your MDP? Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c536e75b992639fa2b21877a388eda0f",
     "grade": true,
     "grade_id": "2b-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For you to make the biggest snowball you will have to be outside. So you will have monitor your temperature. We can define the states you can be in as follows: { 'Cold','Warm','Frozen', 'Toasty'}. There is one terminal states of being Frozen. In the case of being frozen, you were cold for to long and passed out from it. Embarrasingly, you had to be rescued by the fire departement and given hot coco. Hence, you could not finish making the biggest snowball. States cold,warm,toasty are non-terminal states. The non-terminal state of being cold is due to the fact you are adding to the giant snowball and are not by the heater(obviously the heater will melt the snowball!).If you sit by the heater you are just warm. If you sit by the heater for too long, you become too warm. You lose inspiration so you take a 30 minute break inside your house which makes you 'toasty'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05b9b36293b64ab2d3ba757666ac3427",
     "grade": false,
     "grade_id": "2c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2c'></a>\n",
    "\n",
    "## (2c) - 4 points\n",
    "\n",
    "What is the reward structure associated with your MDP?  Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b36dc1c0b6d151da8f42c1d93b9d9a",
     "grade": true,
     "grade_id": "2c-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For this MDP, the reward for being in the state 'Cold' is +5 as you must be by snowball,which means the only action that allows us to stay/go to the state of 'Cold is by adding a piece of snow. R('Warm') will be -1 as we must be by the heater hence no progress would have been made by the last action. As discussed in 2b, the state 'frozen' is if you pass out from hyperthermia and have to be rescued by the fire department. Since you feel this is an embarrasing ordeal and it is a terminal state (we can not finish the snowball) the reward is very negative. R('Frozen') = -100. For the state of 'Toasty', R('Toasty')= -15, as it is our own laziness and lack of determination that is slowing us down.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "565e36ab6960052d86fa2a08c18f96c4",
     "grade": false,
     "grade_id": "2d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2d'></a>\n",
    "\n",
    "## (2d) - 4 points\n",
    "\n",
    "What are the actions and transition model associated with your MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b16a36bb225d610a1837822e543162f1",
     "grade": true,
     "grade_id": "2d-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "There are two actions we can make {'Add Snow to Snowball', 'Rest by Heater'}. For our transition function, we should define which states can go to other states. You can only become 'frozen' from the state 'cold'.I wanted to put this first as for any non-adjacent states will be assigned with probability of 0.\n",
    "\n",
    "Now for the transition model, to move from a state $s$ to $s'$ with probability of .7. The other .3 is distributed to other three states. However, the distribution of the .3 will be split unevenly between the non-terminal states and one terminal state. The non-terminal states will split .8 of the .3 and the other .2 of the .3 will go to the terminal state 'frozen'. Assuming all states are accessible from current state. If the terminal-state is not accessible from the current state than assume the .3 will be distributed to other two non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35ab2fc9b4eb20ce1b32e1a2aa0e1007",
     "grade": false,
     "grade_id": "2e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2e'></a>\n",
    "\n",
    "## (2e) - 4 points\n",
    "\n",
    "Interpret what an optimal policy represents in the context of your particular MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c57bad2508d2aa53541aaaa24b5f55c3",
     "grade": true,
     "grade_id": "2e-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The optimal policy in terms of the MDP, would to be add snow when we are in the warm state as we would most likely end up in the cold state. From the state cold, it is best to rest by the heater as this action would exclude the possibility of getting a huge negative reward in the state s'. From the state 'toasty', the optimal policy would be to add Snow as there is a positive reward for building the snowball. Also, resting by the heater ('warm) would prove to produce a small negative reward instead. Again the goal would be to maximize the size of the snowball and minimize the change of getting in to the 'frozen' terminal state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "519px",
    "left": "22px",
    "top": "149px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
